"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[47323],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=r,f=u["".concat(l,".").concat(d)]||u[d]||m[d]||i;return n?a.createElement(f,o(o({ref:t},p),{},{components:n})):a.createElement(f,o({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},22942:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const i={id:"index",title:"Getting Started with ZIO Kafka",sidebar_label:"Getting Started"},o=void 0,s={unversionedId:"index",id:"index",title:"Getting Started with ZIO Kafka",description:"Contents",source:"@site/node_modules/@zio.dev/zio-kafka/index.md",sourceDirName:".",slug:"/",permalink:"/zio-kafka/",draft:!1,tags:[],version:"current",frontMatter:{id:"index",title:"Getting Started with ZIO Kafka",sidebar_label:"Getting Started"},sidebar:"sidebar",next:{title:"Annotated Tests",permalink:"/zio-kafka/annotated-tests"}},l={},c=[{value:"Contents",id:"contents",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"Consuming Kafka topics using ZIO Streams",id:"consuming-kafka-topics-using-zio-streams",level:2},{value:"Example: consuming, producing and committing offset",id:"example-consuming-producing-and-committing-offset",level:2},{value:"Partition assignment and offset retrieval",id:"partition-assignment-and-offset-retrieval",level:2},{value:"Custom data type serdes",id:"custom-data-type-serdes",level:2},{value:"Handling deserialization failures",id:"handling-deserialization-failures",level:2}],p={toc:c};function m(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"contents"},"Contents"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#quickstart"},"Quickstart")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#consuming-kafka-topics-using-zio-streams"},"Consuming Kafka topics using ZIO Streams")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#example-consuming-producing-and-committing-offset"},"Example: consuming, producing and committing offset")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#partition-assignment-and-offset-retrieval"},"Partition assignment and offset retrieval")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#custom-data-type-serdes"},"Custom data type serdes")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#handling-deserialization-failures"},"Handling deserialization failures"))),(0,r.kt)("h2",{id:"quickstart"},"Quickstart"),(0,r.kt)("p",null,"Add the following dependencies to your ",(0,r.kt)("inlineCode",{parentName:"p"},"build.sbt")," file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'libraryDependencies += "dev.zio" %% "zio-kafka" % "2.0.1"\n')),(0,r.kt)("p",null,"Somewhere in your application, configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"zio.kafka.ConsumerSettings"),"\ndata type:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.kafka.consumer._\n\nval settings: ConsumerSettings = \n  ConsumerSettings(List("localhost:9092"))\n    .withGroupId("group")\n    .withClientId("client")\n    .withCloseTimeout(30.seconds)\n')),(0,r.kt)("p",null,"For a lot of use cases where you just want to do something with all messages on a Kafka topic, ZIO Kafka provides the convenience method ",(0,r.kt)("inlineCode",{parentName:"p"},"Consumer.consumeWith"),". This method lets you execute a ZIO effect for each message. Topic partitions will be processed in parallel and offsets are committed after running the effect automatically."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval subscription = Subscription.topics("topic")\n\nConsumer.consumeWith(settings, subscription, Serde.string, Serde.string) { case (key, value) =>\n  Console.printLine(s"Received message ${key}: ${value}")\n  // Perform an effect with the received message\n}\n')),(0,r.kt)("p",null,"If you require more control over the consumption process, read on!"),(0,r.kt)("h2",{id:"consuming-kafka-topics-using-zio-streams"},"Consuming Kafka topics using ZIO Streams"),(0,r.kt)("p",null,"First, create a consumer using the ConsumerSettings instance:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.Clock, zio.ZLayer, zio.ZManaged\nimport zio.kafka.consumer.{ Consumer, ConsumerSettings }\n\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval consumerManaged: ZIO[Scope, Throwable, Consumer] =\n  Consumer.make(consumerSettings)\nval consumer: ZLayer[Clock, Throwable, Consumer] =\n  ZLayer.scoped(consumerManaged)\n')),(0,r.kt)("p",null,"The consumer returned from ",(0,r.kt)("inlineCode",{parentName:"p"},"Consumer.make")," is wrapped in a ",(0,r.kt)("inlineCode",{parentName:"p"},"ZLayer"),"\nto allow for easy composition with other ZIO environment components.\nYou may provide that layer to effects that require a consumer. Here's\nan example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval data: RIO[Clock, \n              Chunk[CommittableRecord[String, String]]] = \n  (Consumer.subscribe(Subscription.topics("topic")) *>\n  Consumer.plainStream(Serde.string, Serde.string).take(50).runCollect)\n    .provideSomeLayer(consumer)\n')),(0,r.kt)("p",null,"You may stream data from Kafka using the ",(0,r.kt)("inlineCode",{parentName:"p"},"subscribeAnd")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"plainStream"),"\nmethods:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.Clock, zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.subscribeAnd(Subscription.topics("topic150"))\n  .plainStream(Serde.string, Serde.string)\n  .tap(cr => printLine(s"key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n')),(0,r.kt)("p",null,"If you need to distinguish between the different partitions assigned\nto the consumer, you may use the ",(0,r.kt)("inlineCode",{parentName:"p"},"Consumer#partitionedStream")," method,\nwhich creates a nested stream of partitions:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.Clock, zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.subscribeAnd(Subscription.topics("topic150"))\n  .partitionedStream(Serde.string, Serde.string)\n  .tap(tpAndStr => printLine(s"topic: ${tpAndStr._1.topic}, partition: ${tpAndStr._1.partition}"))\n  .flatMap(_._2)\n  .tap(cr => printLine(s"key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n')),(0,r.kt)("h2",{id:"example-consuming-producing-and-committing-offset"},"Example: consuming, producing and committing offset"),(0,r.kt)("p",null,"This example shows how to consume messages from topic ",(0,r.kt)("inlineCode",{parentName:"p"},"topic_a")," and produce transformed messages to ",(0,r.kt)("inlineCode",{parentName:"p"},"topic_b"),", after which consumer offsets are committed. Processing is done in chunks using ",(0,r.kt)("inlineCode",{parentName:"p"},"ZStreamChunk")," for more efficiency."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.ZLayer\nimport zio.kafka.consumer._\nimport zio.kafka.producer._\nimport zio.kafka.serde._\nimport org.apache.kafka.clients.producer.ProducerRecord\n\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval producerSettings: ProducerSettings = ProducerSettings(List("localhost:9092"))\n\nval consumerAndProducer = \n  ZLayer.scoped(Consumer.make(consumerSettings)) ++\n    ZLayer.scoped(Producer.make(producerSettings, Serde.int, Serde.string))\n\nval consumeProduceStream = Consumer\n  .subscribeAnd(Subscription.topics("my-input-topic"))\n  .plainStream(Serde.int, Serde.long)\n  .map { record =>\n    val key: Int    = record.record.key()\n    val value: Long = record.record.value()\n    val newValue: String = value.toString\n\n    val producerRecord: ProducerRecord[Int, String] = new ProducerRecord("my-output-topic", key, newValue)\n    (producerRecord, record.offset)\n  }\n  .mapChunksZIO { chunk =>\n    val records     = chunk.map(_._1)\n    val offsetBatch = OffsetBatch(chunk.map(_._2).toSeq)\n\n    Producer.produceChunk[Any, Int, String](records) *> offsetBatch.commit.as(Chunk(()))\n  }\n  .runDrain\n  .provideSomeLayer(consumerAndProducer)\n')),(0,r.kt)("h2",{id:"partition-assignment-and-offset-retrieval"},"Partition assignment and offset retrieval"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"zio-kafka")," offers several ways to control which Kafka topics and partitions are assigned to your application."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Use case"),(0,r.kt)("th",{parentName:"tr",align:null},"Method"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"One or more topics, automatic partition assignment"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},'Consumer.subscribe(Subscription.topics("my_topic", "other_topic"))'))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Topics matching a pattern"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},'Consumer.subscribe(Subscription.pattern("topic.*"))'))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Manual partition assignment"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},'Consumer.subscribe(Subscription.manual("my_topic" -> 1, "my_topic" -> 2))'))))),(0,r.kt)("p",null,"By default ",(0,r.kt)("inlineCode",{parentName:"p"},"zio-kafka")," will start streaming a partition from the last committed offset for the consumer group, or the latest message on the topic if no offset has yet been committed. You can also choose to store offsets outside of Kafka. This can be useful in cases where consistency between data stores and consumer offset is required."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Use case"),(0,r.kt)("th",{parentName:"tr",align:null},"Method"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Offsets in Kafka, start at latest message if no offset committed"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"OffsetRetrieval.Auto()"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Offsets in Kafka, start at earliest message if no offset committed"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"OffsetRetrieval.Auto(AutoOffsetStrategy.Earliest)"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Manual/external offset storage"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Manual(getOffsets: Set[TopicPartition] => Task[Map[TopicPartition, Long]])"))))),(0,r.kt)("p",null,"For manual offset retrieval, the ",(0,r.kt)("inlineCode",{parentName:"p"},"getOffsets")," function will be called for each topic-partition that is assigned to the consumer, either via Kafka's rebalancing or via a manual assignment."),(0,r.kt)("h2",{id:"custom-data-type-serdes"},"Custom data type serdes"),(0,r.kt)("p",null,"Serializers and deserializers (serdes) for custom data types can be constructed from scratch or by converting existing serdes. For example, to create a serde for an ",(0,r.kt)("inlineCode",{parentName:"p"},"Instant"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"import java.time.Instant\nimport zio.kafka.serde._\n\nval instantSerde: Serde[Any, Instant] = Serde.long.inmap(java.time.Instant.ofEpochMilli)(_.toEpochMilli)\n")),(0,r.kt)("h2",{id:"handling-deserialization-failures"},"Handling deserialization failures"),(0,r.kt)("p",null,"The default behavior for a consumer stream when encountering a deserialization failure is to fail the stream. In many cases you may want to handle this situation differently, e.g. by skipping the message that failed to deserialize or by executing an alternative effect. For this purpose, any ",(0,r.kt)("inlineCode",{parentName:"p"},"Deserializer[T]")," for some type ",(0,r.kt)("inlineCode",{parentName:"p"},"T")," can be easily converted into a ",(0,r.kt)("inlineCode",{parentName:"p"},"Deserializer[Try[T]]")," where deserialization failures are converted to a ",(0,r.kt)("inlineCode",{parentName:"p"},"Failure")," using the ",(0,r.kt)("inlineCode",{parentName:"p"},"asTry")," method."),(0,r.kt)("p",null,"Below is an example of skipping messages that fail to deserialize. The offset is passed downstream to be committed."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import zio._, stream._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\nimport scala.util.{Try, Success, Failure}\n\nval consumer = ZLayer.scoped(Consumer.make(consumerSettings))\n\nval stream = Consumer\n  .subscribeAnd(Subscription.topics("topic150"))\n  .plainStream(Serde.string, Serde.string.asTry)\n\nstream \n  .mapZIO { record => \n    val tryValue: Try[String] = record.record.value()\n    val offset: Offset = record.offset\n  \n    tryValue match {\n      case Success(value) =>\n        // Action for successful deserialization\n        someEffect(value).as(offset)\n      case Failure(exception) =>\n        // Possibly log the exception or take alternative action\n        ZIO.succeed(offset)\n    }\n  }\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n  .provideSomeLayer(consumer)\n')))}m.isMDXComponent=!0}}]);